{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander\n",
    "Using reinforcement learning to teach an agent to play the Lunar Lander game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T18:32:56.975983Z",
     "start_time": "2019-01-05T18:32:56.743116Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T18:32:56.979987Z",
     "start_time": "2019-01-05T18:32:56.977337Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T18:32:56.991097Z",
     "start_time": "2019-01-05T18:32:56.981335Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=1000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v.to(device)))\n",
    "            act_probs = act_probs_v.cpu().data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "            new_s, r, done, info = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T18:32:57.007694Z",
     "start_time": "2019-01-05T18:32:56.992680Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_batch(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T04:50:33.232127Z",
     "start_time": "2019-01-06T04:50:33.225492Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T04:50:34.234026Z",
     "start_time": "2019-01-06T04:50:34.224344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T18:58:42.613867Z",
     "start_time": "2019-01-05T18:32:57.067451Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.363, reward_mean=-159.6, reward_threshold=-98.7\n",
      "1: loss=1.375, reward_mean=-163.7, reward_threshold=-101.6\n",
      "2: loss=1.371, reward_mean=-176.4, reward_threshold=-110.8\n",
      "3: loss=1.378, reward_mean=-149.1, reward_threshold=-96.5\n",
      "4: loss=1.380, reward_mean=-142.6, reward_threshold=-87.0\n",
      "5: loss=1.385, reward_mean=-157.8, reward_threshold=-99.4\n",
      "6: loss=1.376, reward_mean=-140.8, reward_threshold=-83.0\n",
      "7: loss=1.379, reward_mean=-138.0, reward_threshold=-95.2\n",
      "8: loss=1.377, reward_mean=-128.0, reward_threshold=-79.0\n",
      "9: loss=1.376, reward_mean=-133.4, reward_threshold=-82.3\n",
      "10: loss=1.373, reward_mean=-135.5, reward_threshold=-85.8\n",
      "11: loss=1.364, reward_mean=-131.4, reward_threshold=-76.8\n",
      "12: loss=1.372, reward_mean=-134.0, reward_threshold=-81.9\n",
      "13: loss=1.358, reward_mean=-136.2, reward_threshold=-79.4\n",
      "14: loss=1.356, reward_mean=-123.8, reward_threshold=-65.2\n",
      "15: loss=1.345, reward_mean=-127.9, reward_threshold=-68.1\n",
      "16: loss=1.346, reward_mean=-133.2, reward_threshold=-73.1\n",
      "17: loss=1.334, reward_mean=-142.5, reward_threshold=-70.4\n",
      "18: loss=1.328, reward_mean=-138.8, reward_threshold=-63.1\n",
      "19: loss=1.315, reward_mean=-132.2, reward_threshold=-62.7\n",
      "20: loss=1.333, reward_mean=-120.3, reward_threshold=-60.4\n",
      "21: loss=1.324, reward_mean=-126.8, reward_threshold=-66.8\n",
      "22: loss=1.298, reward_mean=-131.8, reward_threshold=-49.6\n",
      "23: loss=1.301, reward_mean=-131.0, reward_threshold=-48.4\n",
      "24: loss=1.291, reward_mean=-143.1, reward_threshold=-63.3\n",
      "25: loss=1.301, reward_mean=-113.4, reward_threshold=-39.1\n",
      "26: loss=1.273, reward_mean=-121.7, reward_threshold=-53.0\n",
      "27: loss=1.270, reward_mean=-112.0, reward_threshold=-40.9\n",
      "28: loss=1.268, reward_mean=-115.1, reward_threshold=-39.5\n",
      "29: loss=1.262, reward_mean=-114.5, reward_threshold=-46.1\n",
      "30: loss=1.276, reward_mean=-104.3, reward_threshold=-32.3\n",
      "31: loss=1.289, reward_mean=-114.0, reward_threshold=-35.6\n",
      "32: loss=1.234, reward_mean=-103.1, reward_threshold=-37.8\n",
      "33: loss=1.265, reward_mean=-112.6, reward_threshold=-33.9\n",
      "34: loss=1.257, reward_mean=-98.4, reward_threshold=-33.0\n",
      "35: loss=1.228, reward_mean=-102.1, reward_threshold=-18.1\n",
      "36: loss=1.248, reward_mean=-89.4, reward_threshold=-21.5\n",
      "37: loss=1.251, reward_mean=-84.5, reward_threshold=-21.9\n",
      "38: loss=1.265, reward_mean=-91.4, reward_threshold=-26.7\n",
      "39: loss=1.260, reward_mean=-80.8, reward_threshold=-14.8\n",
      "40: loss=1.261, reward_mean=-78.1, reward_threshold=-13.6\n",
      "41: loss=1.242, reward_mean=-81.4, reward_threshold=-14.0\n",
      "42: loss=1.181, reward_mean=-65.5, reward_threshold=-1.9\n",
      "43: loss=1.190, reward_mean=-67.9, reward_threshold=-5.5\n",
      "44: loss=1.207, reward_mean=-85.9, reward_threshold=-15.0\n",
      "45: loss=1.253, reward_mean=-83.8, reward_threshold=-11.2\n",
      "46: loss=1.244, reward_mean=-70.5, reward_threshold=-3.1\n",
      "47: loss=1.252, reward_mean=-65.1, reward_threshold=-10.5\n",
      "48: loss=1.208, reward_mean=-74.2, reward_threshold=-10.3\n",
      "49: loss=1.208, reward_mean=-44.3, reward_threshold=15.7\n",
      "50: loss=1.238, reward_mean=-58.3, reward_threshold=7.7\n",
      "51: loss=1.208, reward_mean=-63.0, reward_threshold=0.6\n",
      "52: loss=1.231, reward_mean=-64.2, reward_threshold=-5.9\n",
      "53: loss=1.236, reward_mean=-63.2, reward_threshold=-2.6\n",
      "54: loss=1.187, reward_mean=-52.1, reward_threshold=1.2\n",
      "55: loss=1.185, reward_mean=-48.5, reward_threshold=1.3\n",
      "56: loss=1.225, reward_mean=-58.8, reward_threshold=1.9\n",
      "57: loss=1.241, reward_mean=-54.2, reward_threshold=4.0\n",
      "58: loss=1.212, reward_mean=-44.3, reward_threshold=8.5\n",
      "59: loss=1.216, reward_mean=-33.6, reward_threshold=20.3\n",
      "60: loss=1.191, reward_mean=-16.5, reward_threshold=16.3\n",
      "61: loss=1.237, reward_mean=-13.5, reward_threshold=21.4\n",
      "62: loss=1.230, reward_mean=-10.9, reward_threshold=19.5\n",
      "63: loss=1.221, reward_mean=-38.7, reward_threshold=16.0\n",
      "64: loss=1.232, reward_mean=-29.3, reward_threshold=19.4\n",
      "65: loss=1.226, reward_mean=-27.3, reward_threshold=17.2\n",
      "66: loss=1.239, reward_mean=-22.8, reward_threshold=17.2\n",
      "67: loss=1.209, reward_mean=-11.6, reward_threshold=25.8\n",
      "68: loss=1.223, reward_mean=-20.4, reward_threshold=20.0\n",
      "69: loss=1.242, reward_mean=-24.7, reward_threshold=19.1\n",
      "70: loss=1.235, reward_mean=-23.4, reward_threshold=18.3\n",
      "71: loss=1.221, reward_mean=-18.2, reward_threshold=23.3\n",
      "72: loss=1.198, reward_mean=-8.9, reward_threshold=31.8\n",
      "73: loss=1.233, reward_mean=-17.9, reward_threshold=22.0\n",
      "74: loss=1.212, reward_mean=-13.2, reward_threshold=29.7\n",
      "75: loss=1.212, reward_mean=-22.8, reward_threshold=24.4\n",
      "76: loss=1.229, reward_mean=-14.0, reward_threshold=27.4\n",
      "77: loss=1.240, reward_mean=-13.8, reward_threshold=26.3\n",
      "78: loss=1.232, reward_mean=-13.7, reward_threshold=32.3\n",
      "79: loss=1.245, reward_mean=-9.8, reward_threshold=26.0\n",
      "80: loss=1.213, reward_mean=-8.5, reward_threshold=31.1\n",
      "81: loss=1.223, reward_mean=0.8, reward_threshold=38.1\n",
      "82: loss=1.208, reward_mean=-21.2, reward_threshold=25.5\n",
      "83: loss=1.257, reward_mean=-6.2, reward_threshold=32.7\n",
      "84: loss=1.244, reward_mean=1.3, reward_threshold=36.4\n",
      "85: loss=1.255, reward_mean=11.1, reward_threshold=36.9\n",
      "86: loss=1.257, reward_mean=7.9, reward_threshold=34.1\n",
      "87: loss=1.206, reward_mean=2.9, reward_threshold=31.6\n",
      "88: loss=1.232, reward_mean=4.0, reward_threshold=29.9\n",
      "89: loss=1.252, reward_mean=2.8, reward_threshold=36.5\n",
      "90: loss=1.268, reward_mean=11.4, reward_threshold=37.0\n",
      "91: loss=1.268, reward_mean=9.6, reward_threshold=39.2\n",
      "92: loss=1.256, reward_mean=6.7, reward_threshold=28.9\n",
      "93: loss=1.262, reward_mean=12.6, reward_threshold=39.5\n",
      "94: loss=1.281, reward_mean=13.2, reward_threshold=45.1\n",
      "95: loss=1.274, reward_mean=12.4, reward_threshold=37.8\n",
      "96: loss=1.258, reward_mean=9.2, reward_threshold=34.0\n",
      "97: loss=1.269, reward_mean=11.5, reward_threshold=40.2\n",
      "98: loss=1.267, reward_mean=12.4, reward_threshold=43.4\n",
      "99: loss=1.266, reward_mean=8.9, reward_threshold=46.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "session_size = 100\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "learning_rate = 0.0025\n",
    "completion_score = 200\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "net = net.to(device)\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states, batch_actions, batch_rewards = generate_batch(\n",
    "        env, batch_size, t_max=5000)\n",
    "    elite_states, elite_actions = filter_batch(batch_states, batch_actions,\n",
    "                                               batch_rewards, percentile)\n",
    "\n",
    "#     elite_states = torch.Tensor(elite_states).to(device)\n",
    "#     elite_actions = torch.Tensor(elite_actions).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states).to(device)\n",
    "    tensor_actions = torch.LongTensor(elite_actions).to(device)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_threshold=%.1f\" %\n",
    "          (i, loss_v.item(), mean_reward, threshold))\n",
    "\n",
    "    #check if\n",
    "    if np.mean(batch_rewards) > completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(net.state_dict(), \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T18:58:42.617489Z",
     "start_time": "2019-01-05T18:58:42.615575Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gym.wrappers\n",
    "# env = gym.wrappers.Monitor(\n",
    "#     gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "# generate_batch(env, 1, t_max=5000)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T04:48:05.667988Z",
     "start_time": "2019-01-06T04:48:05.619118Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-da32592a1eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ms_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "# # lets run and see\n",
    "\n",
    "# s = env.reset()\n",
    "# totalReward = 0\n",
    "# activation = nn.Softmax(dim=1)\n",
    "# for _ in range(5000):\n",
    "#     env.render()\n",
    "#     s_v = torch.FloatTensor([s])\n",
    "#     act_probs_v = activation(net(s_v.to(device)))\n",
    "#     act_probs = act_probs_v.cpu().data.numpy()[0]\n",
    "#     a = np.argmax(act_probs)\n",
    "#     new_s, r, done, info = env.step(a)\n",
    "#     totalReward += r\n",
    "    \n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# print(\"Total reward is : {}\".format(totalReward))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
